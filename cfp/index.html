<!DOCTYPE html>
<html lang='en'>

<head>
    <base href='..'>
    <script src="https://code.jquery.com/jquery-3.4.1.js"></script>
    <script> $.get('head.html', function (data) {
            $('head').append(data);
        }) </script>
</head>



<body>

    <div id="header"></div>
    <script> $("#header").load("header.html", function () {
            $("#cfp_button").attr("class", "current");
        }) </script>

    <br>
    <h2></h2>
    <p>
        Multimodal research focuses on collecting resources, developing models,
        and evaluating systems that need to jointly reason over multiple sources of data. Recently, a few works have
        started to exploit the synchronization of multimodal streams to improve prediction of patient status or response
        to treatment in cognitive and mental health applications. This workshop encourages and promotes research efforts
        towards more inclusive multimodal technologies for cognitive and mental health applications and tools to assess
        those methods. The goal of the workshop is to foster research combining text and speech, with other multimodal
        inputs, including images, videos, wearable devices, survey, structured, spatio-temporal data, and knowledge
        bases. We invite papers which focus on the topics of interest include (but is not limited to):
    <ul>
        <li>Multimodal or cross-modal learning for cognitive or mental health</li>
        <li>Datasets for multimodal machine learning for cognitive or mental health</li>
        <li>Multi-task multimodal learning for cognitive or mental health</li>
        <li>Evaluation and analysis of multimodal models for cognitive or mental health</li>
        <li>Interpretability of multimodal architectures for cognitive or mental health</li>
        <li>Bias in multimodal learning for cognitive or mental health</li>
        <li>Multilingual multimodal machine learning for cognitive or mental health</li>
        <li>Multimodal fusion combining imaging, language and speech data</li>
        <li>Multimodal fusion combining genomic data, language and speech data</li>
        <li>Multimodal fusion combining spatial-temporal, language and speech data</li>
        <li>Multimodal disease classification and prediction</li>
        <li>Multimodal biomarkers for measuring response to treatments</li>
        <li>Multimodal disease model-building and clinical decision support</li>
        <li>Multimodal distributed and federated learning for cognitive or mental health applications</li>

    </ul>

    <p>
        We are inviting submission of short papers (4 pages) or long papers (6 pages). Authors can submit an unlimited
        number of pages for references and supplementary material, but supplementary material will not necessarily be
        reviewed. All submissions must be fully anonymized to preserve the double-blind reviewing policy. Insufficiently
        anonymized submissions will be considered for desk-reject.
    </p>

    <!-- <h2>Submission</h2>
    <p>
        Submit your one page summary online via the 2023 MASC-SLL Openreview site

        <a
            href="https://openreview.net/group?id=MASC-SLL/2023/Colloquium">https://openreview.net/group?id=MASC-SLL/2023/Colloquium</a>
    </p>
    <p>
        Summaries should be no more than one page, not including references. If an under-review or published paper has
        more than one page, the authors need to submit a one-page version of the paper and specify as a footnote the
        name of the conference at which the original work is published or under review. You should summarize the
        motivation, contributions, methods, and evaluation schemes of the research conducted by the authors. </p>
    <p>
        Summaries should conform to ACL style guidelines. LaTeX style files, along with a style guide, are available at
        <a href="https://github.com/acl-org/acl-style-files">https://github.com/acl-org/acl-style-files</a>. Authors may
        NOT need a separate "abstract" section in your submission. Please include all names and affiliations in the
        summary.
    </p>
    <p>
        Summaries reviewing will NOT be blind. All accepted summaries will be non-archival; authors can submit them to
        subsequent conferences.
    </p>
    <p>
        <strong>Cross-Submission Policy</strong>: MASC-SLL 2023 welcomes submissions that are under review or have been
        published at recent scientific conferences. Please double-check the policies of those conferences to make sure
        that submitting to non-archival venues is allowed. At the time of submission, we will ask the authors to
        indicate whether the submission is unpublished, or is a short version of an under-review or published paper.
    </p>

    <h2>Important Dates</h2>
    <ul>
        <li> Submission opens: February 17 </li>
        <li> Submission deadline: <s>March 17</s> March 24</li>
        <li> Decisions announced: <s>March 31</s> April 3</li>
        <li> Registration opens: <s>March 17</s> March 24</li>
        <li> Registration closes: April 10 </li>
        <li> Colloquium: April 22 </li>
    </ul>
    <p> All deadlines are at 11:59pm EST (GMT-5).

    <h2>Topics</h2>
    <p> Relevant topics include but are not restricted to the following:</p>
    <ul>
        <li>Computational Models of Human Language Processing</li>
        <li>Computational Phonology and Morphology</li>
        <li>Computational Social Science and Cultural Analytics</li>
        <li>Dialog and Discourse</li>
        <li>Dialogue and Interactive Systems</li>
        <li>Discourse and Pragmatics</li>
        <li>Efficient Methods for NLP</li>
        <li>Ethics and NLP</li>
        <li>Fairness and Bias in Speech and Language</li>
        <li>Generation</li>
        <li>Information Extraction</li>
        <li>Information Retrieval and Web Search</li>
        <li>Interpretability and Analysis of Models for NLP</li>
        <li>Question Answering and Reading Comprehension</li>
        <li>Knowledge Base Population and Machine Reading</li>
        <li>Language Acquisition</li>
        <li>Language Disorders</li>
        <li>Language Generation and Summarization</li>
        <li>Language Grounding to Vision, Robotics and Beyond</li>
        <li>Language Resources and Annotation</li>
        <li>Lexical Semantics and Ontologies</li>
        <li>Linguistic theories, Cognitive Modeling and Psycholinguistics</li>
        <li>Low-resource Language Processing</li>
        <li>Machine Learning for NLP and Speech</li>
        <li>Machine Translation</li>
        <li>Multilingual/Cross-Lingual Processing</li>
        <li>Multimodal and Interactive Language or Speech Learning</li>
        <li>NLP Applications</li>
        <li>NLP for the Web and Social Media</li>
        <li>Phonetics, Phonology, Morphology, Word Segmentation and Prosody</li>
        <li>Paralinguistics in Speech and Language</li>
        <li>Question Answering</li>
        <li>Resources and Evaluation</li>
        <li>Semantics</li>
        <li>Sentiment Analysis, Opinion Mining, Stylistic Analysis, and Argument Mining</li>
        <li>Speaker Variability</li>
        <li>Speech Perception, Production and Acquisition</li>
        <li>Speech Recognition and Synthesis</li>
        <li>Syntax: Tagging, Chunking and Parsing</li>
        <li>Text and Document Classification</li>
        <li>Text Mining and Information Extraction</li>
    </ul>
    <h2>Contact</h2>
    <p> For any questions relevant to the event, please reach out to mascsll2023@gmail.com. Stay tuned with us at <a
            href="https://www.mascsll.org/">https://www.mascsll.org/! </a></p> -->


    <div id="footer"></div>
    <script> $("#footer").load("footer.html") </script>

</body>

</body>

</html>